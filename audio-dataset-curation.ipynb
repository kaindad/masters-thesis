{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ei-audio-dataset-curation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaindad/masters-thesis/blob/main/audio-dataset-curation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAn75H1iPYOD"
      },
      "source": [
        "### Upload the clean Medndeley data samples\n",
        "\n",
        "\n",
        "\n",
        "The audio samples should be `.wav` format, mono, and 1 second long. Bitrate and bitdepth should not matter. Samples shorter than 1 second will be padded with 0s, and samples longer than 1 second will be truncated to 1 second. The exact name of each `.wav` matter, as they will be read, mixed with background noise, and saved to a separate file with an auto-generated name. Directory name does matter (it is used to determine the name of the class during neural network training).\n",
        "\n",
        "Right-click on each keyword directory and upload all of your samples. Your directory structor should look like the following:\n",
        "\n",
        "```\n",
        "/\n",
        "|- chicken-data-healthy-combined-clean.wav\n",
        "|- chicken-data-noise-combined-clean.wav\n",
        "|- chicken-data-unhealthy-combined-clean.wav\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "uezrQvmZ7oa7",
        "outputId": "6c0e38fb-31c6-42c1-9fd7-6956f72987f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "\n",
        "class WavFileSplitter():\n",
        "    def __init__(self, source_filename):\n",
        "        self.current_directory = os.getcwd()  # Get the current directory\n",
        "        self.source_filename = source_filename\n",
        "        self.source_filepath = os.path.join(self.current_directory, source_filename)\n",
        "\n",
        "        self.audio_segment = AudioSegment.from_wav(self.source_filepath)\n",
        "\n",
        "    def _calculate_audio_duration(self):\n",
        "        return self.audio_segment.duration_seconds\n",
        "\n",
        "    def _export_audio_slice(self, start_second, end_second, output_filename, destination_directory):\n",
        "        start_time = start_second * 1000  # Convert to milliseconds\n",
        "        end_time = end_second * 1000  # Convert to milliseconds\n",
        "        audio_slice = self.audio_segment[start_time:end_time]\n",
        "        audio_slice.export(os.path.join(destination_directory, output_filename), format=\"wav\")\n",
        "\n",
        "    def split_audio_into_intervals(self, seconds_per_slice, output_prefix):\n",
        "        destination_directory = os.path.join(self.current_directory, output_prefix)\n",
        "        if not os.path.exists(destination_directory):\n",
        "            os.makedirs(destination_directory)  # Create the directory if it doesn't exist\n",
        "\n",
        "        total_seconds = math.ceil(self._calculate_audio_duration())\n",
        "        for i in range(0, total_seconds, seconds_per_slice):\n",
        "            slice_filename = f\"{output_prefix}_{i+1}.wav\"  # Naming files like prefix_1.wav, prefix_2.wav, ...\n",
        "            self._export_audio_slice(i, i+seconds_per_slice, slice_filename, destination_directory)\n",
        "            print(f\"Exported: {slice_filename}\")\n",
        "            if i == total_seconds - seconds_per_slice:\n",
        "                print('All slices exported successfully')\n",
        "\n",
        "# Example usage:\n",
        "source_filename = \"unhealthy-combined-clean.wav\"\n",
        "audio_splitter = WavFileSplitter(source_filename)\n",
        "audio_splitter.split_audio_into_intervals(1, \"unhealthy\")  # This will split the WAV file into 1-second intervals with the prefix \"unhealthy_slice\""
      ],
      "metadata": {
        "id": "LEVlDGqP3dip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the directory to be compressed and the output compressed filename\n",
        "source_directory = os.path.join(os.getcwd(), \"unhealthy\")\n",
        "output_filename = \"unhealthy_compressed.zip\"\n",
        "\n",
        "# Compress the directory\n",
        "shutil.make_archive(output_filename[:-4], 'zip', source_directory)\n",
        "\n",
        "print(f\"'{source_directory}' has been compressed to '{output_filename}' in the current directory.\")\n"
      ],
      "metadata": {
        "id": "ev1xPesm_xAk",
        "outputId": "deb302bf-0b6e-4dd6-ec0d-f317630f798c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/unhealthy' has been compressed to 'unhealthy_compressed.zip' in the current directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMkAgYc8MuCq"
      },
      "source": [
        "# Keyword Spotting Dataset Curation\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/ei-keyword-spotting/blob/master/ei-audio-dataset-curation.ipynb)\n",
        "\n",
        "Use this tool to download the Google Speech Commands Dataset, combine it with your own keywords, mix in some background noise, and upload the curated dataset to Edge Impulse. From there, you can train a neural network to classify spoken words and upload it to a microcontroller to perform real-time keyword spotting.\n",
        "\n",
        " 1. Upload samples of your own keyword (optional)\n",
        " 2. Adjust parameters in the Settings cell (you will need an [Edge Impulse](https://www.edgeimpulse.com/) account)\n",
        " 3. Run the rest of the cells! ('shift' + 'enter' on each cell)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81cDNtYQj-ao"
      },
      "source": [
        "### Update Node.js to the latest stable version\n",
        "!npm cache clean -f\n",
        "!npm install -g n\n",
        "!n 16.18.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "EmVv0QvV-z_8",
        "outputId": "0b636e9d-a989-401d-e754-1d59d0ebf576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjn7Y0iPXCh"
      },
      "source": [
        "### Install required packages and tools\n",
        "!python -m pip install soundfile\n",
        "!npm install -g --unsafe-perm edge-impulse-cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06YDTU0c0-H"
      },
      "source": [
        "### Settings (You probably do not need to change these)\n",
        "BASE_DIR = \"/content\"\n",
        "OUT_DIR = \"keywords_curated\"\n",
        "GOOGLE_DATASET_FILENAME = \"speech_commands_v0.02.tar.gz\"\n",
        "GOOGLE_DATASET_URL = \"http://download.tensorflow.org/data/\" + GOOGLE_DATASET_FILENAME\n",
        "GOOGLE_DATASET_DIR = \"google_speech_commands\"\n",
        "CUSTOM_KEYWORDS_FILENAME = \"main.zip\"\n",
        "CUSTOM_KEYWORDS_URL = \"https://github.com/ShawnHymel/custom-speech-commands-dataset/archive/\" + CUSTOM_KEYWORDS_FILENAME\n",
        "CUSTOM_KEYWORDS_DIR = \"custom_keywords\"\n",
        "CUSTOM_KEYWORDS_REPO_NAME = \"custom-speech-commands-dataset-main\"\n",
        "CURATION_SCRIPT = \"dataset-curation.py\"\n",
        "CURATION_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/\" + CURATION_SCRIPT\n",
        "UTILS_SCRIPT_URL = \"https://raw.githubusercontent.com/ShawnHymel/ei-keyword-spotting/master/utils.py\"\n",
        "NUM_SAMPLES = 1500    # Target number of samples to mix and send to Edge Impulse\n",
        "WORD_VOL = 1.0        # Relative volume of word in output sample\n",
        "BG_VOL = 0.1          # Relative volume of noise in output sample\n",
        "SAMPLE_TIME = 1.0     # Time (seconds) of output sample\n",
        "SAMPLE_RATE = 16000   # Sample rate (Hz) of output sample\n",
        "BIT_DEPTH = \"PCM_16\"  # Options: [PCM_16, PCM_24, PCM_32, PCM_U8, FLOAT, DOUBLE]\n",
        "BG_DIR = \"_background_noise_\"\n",
        "TEST_RATIO = 0.2      # 20% reserved for test set, rest is for training\n",
        "EI_INGEST_TEST_URL = \"https://ingestion.edgeimpulse.com/api/test/data\"\n",
        "EI_INGEST_TRAIN_URL = \"https://ingestion.edgeimpulse.com/api/training/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVA3SDiQd-jh"
      },
      "source": [
        "### Download Google Speech Commands Dataset\n",
        "!cd {BASE_DIR}\n",
        "!wget {GOOGLE_DATASET_URL}\n",
        "!mkdir {GOOGLE_DATASET_DIR}\n",
        "!echo \"Extracting...\"\n",
        "!tar xfz {GOOGLE_DATASET_FILENAME} -C {GOOGLE_DATASET_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNGNSwX_d_-E"
      },
      "source": [
        "### Pull out background noise directory\n",
        "!cd {BASE_DIR}\n",
        "!mv \"{GOOGLE_DATASET_DIR}/{BG_DIR}\" \"{BG_DIR}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L-vJENeB1S"
      },
      "source": [
        "### (Optional) Download custom dataset--uncomment the code in this cell if you want to use my custom datase\n",
        "\n",
        "## Download, extract, and move dataset to separate directory\n",
        "# !cd {BASE_DIR}\n",
        "# !wget {CUSTOM_KEYWORDS_URL}\n",
        "# !echo \"Extracting...\"\n",
        "# !unzip -q {CUSTOM_KEYWORDS_FILENAME}\n",
        "# !mv \"{CUSTOM_KEYWORDS_REPO_NAME}/{CUSTOM_KEYWORDS_DIR}\" \"{CUSTOM_KEYWORDS_DIR}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nDT4lYEMlY1"
      },
      "source": [
        "### User Settings (do change these)\n",
        "\n",
        "# Location of your custom keyword samples (e.g. \"/content/custom_keywords\")\n",
        "# Leave blank (\"\") for no custom keywords. set to the CUSTOM_KEYWORDS_DIR\n",
        "# variable to use samples from my custom-speech-commands-dataset repo.\n",
        "CUSTOM_DATASET_PATH = \"\"\n",
        "\n",
        "# Edge Impulse > your_project > Dashboard > Keys\n",
        "EI_API_KEY = \"ei_e544...\"\n",
        "\n",
        "# Comma separated words. Must match directory names (that contain samples).\n",
        "# Recommended: use 2 keywords for microcontroller demo\n",
        "TARGETS = \"go, stop\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ve08zgfVLem"
      },
      "source": [
        "### Download curation and utils scripts\n",
        "!wget {CURATION_SCRIPT_URL}\n",
        "!wget {UTILS_SCRIPT_URL}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O-M_hViZ0ew"
      },
      "source": [
        "### Perform curation and mixing of samples with background noise\n",
        "!cd {BASE_DIR}\n",
        "!python {CURATION_SCRIPT} \\\n",
        "  -t \"{TARGETS}\" \\\n",
        "  -n {NUM_SAMPLES} \\\n",
        "  -w {WORD_VOL} \\\n",
        "  -g {BG_VOL} \\\n",
        "  -s {SAMPLE_TIME} \\\n",
        "  -r {SAMPLE_RATE} \\\n",
        "  -e {BIT_DEPTH} \\\n",
        "  -b \"{BG_DIR}\" \\\n",
        "  -o \"{OUT_DIR}\" \\\n",
        "  \"{GOOGLE_DATASET_DIR}\" \\\n",
        "  \"{CUSTOM_DATASET_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXd0wH1-hEEX"
      },
      "source": [
        "### Use CLI tool to send curated dataset to Edge Impulse\n",
        "\n",
        "!cd {BASE_DIR}\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Seed with system time\n",
        "random.seed()\n",
        "\n",
        "# Go through each category in our curated dataset\n",
        "for dir in os.listdir(OUT_DIR):\n",
        "\n",
        "  # Create list of files for one category\n",
        "  paths = []\n",
        "  for filename in os.listdir(os.path.join(OUT_DIR, dir)):\n",
        "    paths.append(os.path.join(OUT_DIR, dir, filename))\n",
        "\n",
        "  # Shuffle and divide into test and training sets\n",
        "  random.shuffle(paths)\n",
        "  num_test_samples = int(TEST_RATIO * len(paths))\n",
        "  test_paths = paths[:num_test_samples]\n",
        "  train_paths = paths[num_test_samples:]\n",
        "\n",
        "  # Create arugments list (as a string) for CLI call\n",
        "  test_paths = ['\"' + s + '\"' for s in test_paths]\n",
        "  test_paths = ' '.join(test_paths)\n",
        "  train_paths = ['\"' + s + '\"' for s in train_paths]\n",
        "  train_paths = ' '.join(train_paths)\n",
        "\n",
        "  # Send test files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category testing \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {test_paths}\n",
        "\n",
        "  # # Send training files to Edge Impulse\n",
        "  !edge-impulse-uploader \\\n",
        "    --category training \\\n",
        "    --label {dir} \\\n",
        "    --api-key {EI_API_KEY} \\\n",
        "    --silent \\\n",
        "    {train_paths}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}